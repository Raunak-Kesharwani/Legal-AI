{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c128d92-c14f-4afd-80b8-8e14bf599622",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyAnOLAWbG1Pipm_fh8fRTVIxoOSOFtWV0I\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bc8ce4f-3618-49c0-ba38-871346c0ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5f737dc-9b7f-4521-b409-d62494d1158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.path = file_path\n",
    "\n",
    "    def _check_file_type(self):\n",
    "        return self.path.split(\".\")[-1].lower()\n",
    "\n",
    "    def load(self):\n",
    "        ext = self._check_file_type()\n",
    "\n",
    "        if ext == \"pdf\":\n",
    "            loader = PyPDFLoader(self.path)\n",
    "        elif ext == \"txt\":\n",
    "            loader = TextLoader(self.path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "        return loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "094d1214-d030-4e4f-8088-7e07debf2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "class DocumentDatabase:\n",
    "    def __init__(self, db_name=\"documents.db\"):\n",
    "        self.conn = sqlite3.connect(db_name)\n",
    "        self.create_table()\n",
    "\n",
    "    def create_table(self):\n",
    "        query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "            document_id TEXT PRIMARY KEY,\n",
    "            created_at TEXT,\n",
    "            updated_at TEXT,\n",
    "            status TEXT,\n",
    "            raw_text TEXT,\n",
    "            final_summary TEXT,\n",
    "            risk_flag TEXT,\n",
    "            chunk_summaries TEXT,\n",
    "            num_chunks INTEGER\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.conn.execute(query)\n",
    "        self.conn.commit()\n",
    "\n",
    "    def insert_document(self, raw_text):\n",
    "        document_id = str(uuid.uuid4())\n",
    "        now = datetime.now().isoformat()\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO documents (\n",
    "            document_id, created_at, updated_at, status,\n",
    "            raw_text, final_summary, risk_flag,\n",
    "            chunk_summaries, num_chunks\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "\n",
    "        self.conn.execute(query, (\n",
    "            document_id,\n",
    "            now,\n",
    "            now,\n",
    "            \"processing\",\n",
    "            raw_text,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            0\n",
    "        ))\n",
    "\n",
    "        self.conn.commit()\n",
    "        return document_id\n",
    "\n",
    "    def update_document(self, document_id, final_summary,\n",
    "                        risk_flag, chunk_summaries, num_chunks):\n",
    "        now = datetime.now().isoformat()\n",
    "\n",
    "        query = \"\"\"\n",
    "        UPDATE documents\n",
    "        SET updated_at = ?,\n",
    "            status = ?,\n",
    "            final_summary = ?,\n",
    "            risk_flag = ?,\n",
    "            chunk_summaries = ?,\n",
    "            num_chunks = ?\n",
    "        WHERE document_id = ?\n",
    "        \"\"\"\n",
    "\n",
    "        self.conn.execute(query, (\n",
    "            now,\n",
    "            \"completed\",\n",
    "            final_summary,\n",
    "            risk_flag,\n",
    "            json.dumps(chunk_summaries),\n",
    "            num_chunks,\n",
    "            document_id\n",
    "        ))\n",
    "\n",
    "        self.conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28a0f3da-8060-4a6f-9a12-6ddb7eb137e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "\n",
    "    def summary_split(self):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2500,\n",
    "            chunk_overlap=150\n",
    "        )\n",
    "        return splitter.split_text(self.text)\n",
    "\n",
    "    def embed_split(self):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size= 1200,\n",
    "            chunk_overlap=150\n",
    "        )\n",
    "        return splitter.split_documents(self.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6a27574-168e-4229-b88b-27d4075acf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class ChunkLegalResponse(BaseModel):\n",
    "    Summary: str = Field(\n",
    "        description=\"Detailed summary of this section\"\n",
    "    )\n",
    "    Flag: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Risky clause if present\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FinalLegalResponse(BaseModel):\n",
    "    Summary: str = Field(\n",
    "        min_length=300,\n",
    "        description=\"Full multi-paragraph legal summary\"\n",
    "    )\n",
    "    Flag: Optional[str] = Field(\n",
    "        description=\"Most significant risky clause for bearer\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49271ce9-1da8-4850-afd6-584556ba1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api = api_key\n",
    "\n",
    "    def get_chat_model(self):\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-pro\", \n",
    "            google_api_key=self.api,\n",
    "            temperature=0.4,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "\n",
    "    def get_embed_model(self):\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model = \"gemini-embedding-001\",\n",
    "            api_key=self.api,\n",
    "            output_dimensionality=512\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3d88365-2d90-44ab-a855-f9cb2b46b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chunk_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a legal document analysis assistant.\n",
    "\n",
    "Analyze ONLY the provided text segment.\n",
    "\n",
    "Tasks:\n",
    "1. Produce a detailed legal summary of this segment.\n",
    "2. Identify any clause that may be risky or harmful to the bearer.\n",
    "\n",
    "Rules:\n",
    "- Do not infer beyond the given text.\n",
    "- If no risky clause exists, explicitly say so.\n",
    "- Preserve exact legal language where relevant.\n",
    "\"\"\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88b02ff5-ee1c-438b-9372-d924dcc0836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a senior legal analyst.\n",
    "\n",
    "You are given multiple legal summaries and identified risks\n",
    "from different sections of the same document.\n",
    "\n",
    "Tasks:\n",
    "1. Produce a cohesive, multi-paragraph legal summary of the ENTIRE document.\n",
    "2. Identify the SINGLE most significant risky clause affecting the bearer.\n",
    "3. If multiple risks exist, choose the most severe.\n",
    "4. If no risks exist, state this clearly.\n",
    "\n",
    "Return output strictly in the required JSON format.\n",
    "\"\"\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d880968a-6ccf-4145-a7b9-f87e75840024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_chunks(chunks: List[str], max_chunks: int = 9) -> List[str]:\n",
    "    \"\"\"\n",
    "    Limit the number of chunks to at most max_chunks\n",
    "    by evenly sampling across the document.\n",
    "    \"\"\"\n",
    "    if len(chunks) <= max_chunks:\n",
    "        return chunks\n",
    "\n",
    "    step = max(1, len(chunks) // max_chunks)\n",
    "    return [chunks[i] for i in range(0, len(chunks), step)][:max_chunks]\n",
    "\n",
    "\n",
    "def truncate_text(text: str, max_chars: int = 8000) -> str:\n",
    "    \"\"\"\n",
    "    Prevent silent LLM failures by bounding input size.\n",
    "    \"\"\"\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    return text[:max_chars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8cd81cc-cbe8-432c-9b30-1141af1f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalSummarizer:\n",
    "    def __init__(self, model, text: str):\n",
    "        self.base_model = model\n",
    "        self.text = text\n",
    "\n",
    "    def summarize(self) -> FinalLegalResponse:\n",
    "        chunks = TextSplitter(self.text).summary_split()\n",
    "\n",
    "        chunks = cap_chunks(chunks, max_chunks=9)\n",
    "\n",
    "        chunk_model = self.base_model.with_structured_output(ChunkLegalResponse)\n",
    "\n",
    "        chunk_results = []\n",
    "        for chunk in chunks:\n",
    "            result = (chunk_prompt | chunk_model).invoke(\n",
    "                {\"text\": chunk}\n",
    "            )\n",
    "            chunk_results.append(result)\n",
    "\n",
    "\n",
    "        merged_input = \"\\n\\n\".join(\n",
    "        f\"SUMMARY:\\n{r.Summary}\\nRISK:\\n{r.Flag or 'None'}\"\n",
    "        for r in chunk_results\n",
    "        )\n",
    "\n",
    "        merged_input = truncate_text(merged_input, max_chars=8000)\n",
    "\n",
    "\n",
    "        final_model = self.base_model.with_structured_output(FinalLegalResponse)\n",
    "\n",
    "        final_result = (merge_prompt | final_model).invoke(\n",
    "            {\"text\": merged_input}\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"final_summary\": final_result.Summary,\n",
    "            \"risk_flag\": final_result.Flag,\n",
    "            \"chunk_summaries\": [\n",
    "                        {\n",
    "                        \"summary\": r.Summary,\n",
    "                        \"risk\": r.Flag\n",
    "                            }\n",
    "                        for r in chunk_results\n",
    "                                    ]\n",
    "                    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0be1c70e-3eb7-45a7-a5b8-5731f62c9456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored Document ID: 2177bd3b-272c-468b-a72f-c7487fad5d3e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6968\\3560838923.py:57: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "# Initialize DB\n",
    "db = DocumentDatabase()\n",
    "\n",
    "docs = DataLoader(\"sample-doc/sample_rent_doc.txt\").load()\n",
    "text = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Insert initial document (status = processing)\n",
    "document_id = db.insert_document(text)\n",
    "\n",
    "model = ChatModel(api_key=GOOGLE_API_KEY).get_chat_model()\n",
    "\n",
    "summarizer = LegalSummarizer(model, text)\n",
    "result = summarizer.summarize()\n",
    "\n",
    "# Prepare chunk summaries safely\n",
    "chunk_summaries = []\n",
    "for idx, r in enumerate(result[\"chunk_summaries\"]):\n",
    "    entry = {\n",
    "        \"chunk_index\": idx,\n",
    "        \"summary\": r[\"summary\"]\n",
    "    }\n",
    "    if r.get(\"risk\"):\n",
    "        entry[\"risk\"] = r[\"risk\"]\n",
    "    chunk_summaries.append(entry)\n",
    "\n",
    "# Update document with final results\n",
    "db.update_document(\n",
    "    document_id=document_id,\n",
    "    final_summary=result[\"final_summary\"],\n",
    "    risk_flag=result[\"risk_flag\"],\n",
    "    chunk_summaries=chunk_summaries,\n",
    "    num_chunks=len(chunk_summaries)\n",
    ")\n",
    "\n",
    "print(\"Stored Document ID:\", document_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df5289-098d-49c9-85fa-f7aa9662d664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e203d-c38d-4496-8207-570cbba8c226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c6e41-f8d7-475a-b79a-f3c29a79b105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841628d-ef4b-43fc-856f-3f50136648cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14707e26-684f-4860-b962-c6e500ba2626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
